{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法\n",
    "\n",
    "## 数値微分の難点\n",
    "\n",
    "前回までで、ニューラルネットワークの基本的な学習方法である、確率的勾配降下法は実装できた\n",
    "\n",
    "このとき、勾配の計算は数値微分によって実装していた\n",
    "\n",
    "数値微分はシンプルに実装できて分かりやすいが、計算に時間がかかるという難点がある\n",
    "\n",
    "そこで、ここでは、重みパラメータの勾配計算を効率良く行う**誤差逆伝播法**を実装する\n",
    "\n",
    "\n",
    "## 計算グラフ\n",
    "\n",
    "誤差逆伝播法を視覚的に理解する方法として**計算グラフ（Computational graph）**というものがある\n",
    "\n",
    "計算グラフとは計算の過程をグラフによって表したものである\n",
    "\n",
    "ここで言うグラフとは、データ構造としてのグラフであり、複数のノードとエッジ（ノード間を結ぶ直線）によって表現されるものである\n",
    "\n",
    "### 計算グラフで計算問題を解く\n",
    "以下のような簡単な計算問題を、計算グラフを使って解いてみる\n",
    "\n",
    "**問:** スーパーで1個100円のりんごを2個買ったとき、支払う金額はいくらか？（消費税は10％とする）\n",
    "\n",
    "計算グラフは以下のように、ノードごとの計算結果が左から右へ伝わるように表現する\n",
    "\n",
    "![computational_graph01.png](./img/computational_graph01.png)\n",
    "\n",
    "上記では、「×2」や「×1.1」を一つの演算として○（ノード）でくくっているが、「×」のような演算子は単一のノードで表現するのが望ましい\n",
    "\n",
    "この場合、「2」と「1.1」は、それぞれ「りんごの個数」と「消費税」という変数として、ノードの外側に表記する\n",
    "\n",
    "![computational_graph02.png](./img/computational_graph02.png)\n",
    "\n",
    "次に、もう少し複雑な計算を計算グラフで解く\n",
    "\n",
    "**問:** スーパーでりんご（100円／個）を2個、みかん（150円／個）を3個買ったとき、支払う金額はいくらか？（消費税は10％とする）\n",
    "\n",
    "この問題を解くための計算グラフは以下のようになる\n",
    "\n",
    "![computational_graph03.png](./img/computational_graph03.png)\n",
    "\n",
    "このように、計算グラフを使って問題を解くには、\n",
    "\n",
    "1. 計算式に使われる要素を演算子と変数に分ける\n",
    "2. 演算子と変数をノードとしてエッジでつなぐ\n",
    "3. 計算グラフ上で計算を左から右へ進める\n",
    "\n",
    "という流れで作業する\n",
    "\n",
    "ここで、「計算を左から右へ進める」処理を**順伝播（forward propagation）**と呼び、ニューラルネットワークの**推論処理**に対応する\n",
    "\n",
    "逆に「計算を右から左へ戻る」処理を**逆伝播（back propagation）**と呼び、ニューラルネットワークの**学習処理**に対応する\n",
    "\n",
    "この逆伝播は、この先、微分を計算するにあたって重要な働きをする\n",
    "\n",
    "### 局所的計算と逆伝播による微分\n",
    "計算グラフを使う利点は大きく以下の2点がある\n",
    "\n",
    "- 「局所的な計算」を伝播することにより複雑な計算を行うことができる\n",
    "    - 各ノードは、全体の計算には関与せず「自分に関係する小さな範囲」の計算だけを行う（局所的計算）\n",
    "    - これにより計算を単純化することができる\n",
    "    - また、計算途中の結果をすべて、各ノードで保持することも可能\n",
    "- 計算グラフを逆伝播することで微分値を効率よく計算できる\n",
    "    - 順伝播が「局所的な計算」であるのと同様に、逆伝播は「局所的な微分」を表す\n",
    "    - これにより微分計算を単純化し、計算速度を向上させることができる\n",
    "\n",
    "例えば上記の問題について、りんごの値段が値上がりした場合、最終的な支払金額にどのように影響するか知りたいとする\n",
    "\n",
    "これは「りんごの値段に対する支払金額の微分」を求めることに相当する（りんごの値段を $x$, 支払金額を $L$ とした場合、$∂L/∂x$ を計算することに相当する）\n",
    "\n",
    "計算グラフの逆伝播によって、この問題を解くと以下のようになる\n",
    "\n",
    "![computational_graph04.png](./img/computational_graph04.png)\n",
    "\n",
    "上記のように、計算グラフを右から左へ計算することで「局所的な微分」を伝達することができる\n",
    "\n",
    "この結果から「りんごの値段に関する支払金額の微分」の値は 2.2 であると言える\n",
    "\n",
    "すなわち、りんごが1円値上がりしたら、最終的な支払金額は2.2円増えることを意味する（正確には、りんごの値段がある微小な値だけ増えたら、最終的な支払金額はその微小な値の2.2倍だけ増加することを意味する）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 連鎖律\n",
    "\n",
    "### 計算グラフの逆伝播\n",
    "計算グラフの逆伝播を一般化すると以下のように表すことができる\n",
    "\n",
    "![computational_graph05.png](./img/computational_graph05.png)\n",
    "\n",
    "ここで、上記計算グラフは $y = f(x)$ という計算を表現している\n",
    "\n",
    "逆伝播の計算手順は、信号 $E$ に対して、ノードの局所的な微分 $\\frac{∂y}{∂x}$ を乗算し、次のノードで伝達する、というものになっている\n",
    "\n",
    "これは、前述したりんごの支払金額計算で考えると分かりやすい\n",
    "\n",
    "```\n",
    "最初のノード: f(x) = 2x (x: 前のノードの出力値＝りんごの値段, 2: りんごの個数)\n",
    "最後のノード: g(x) = 1.1x (x: 前のノードの出力値, 1.1: 消費税倍率)\n",
    "\n",
    "最後のノードの逆伝播: 1 * g'(x) = 1 * 1.1 = 1.1\n",
    "最初のノードの逆伝播: 1.1 * f'(x) = 1.1 * 2 = 2.2\n",
    "```\n",
    "\n",
    "この計算により効率よく微分値を求めることができるだが、その理由は**連鎖律の原理**から説明できる\n",
    "\n",
    "### 連鎖律\n",
    "以下のような計算グラフを考える\n",
    "\n",
    "![computational_graph06.png](./img/computational_graph06.png)\n",
    "\n",
    "この計算を微分すると、以下のような逆伝播で表現される\n",
    "\n",
    "![computational_graph07.png](./img/computational_graph07.png)\n",
    "\n",
    "ここで、合成関数の定理より $\\frac{∂z}{∂z} \\frac{∂z}{∂y}$ の $∂z$ は \"打ち消し合い\"、$\\frac{∂z}{∂y}$ となる\n",
    "\n",
    "同様にして $\\frac{∂z}{∂z} \\frac{∂z}{∂y} \\frac{∂y}{∂x} = \\frac{∂z}{∂x}$ となる\n",
    "\n",
    "このような合成関数の微分の性質を**連鎖律**と呼ぶ\n",
    "\n",
    "連鎖律により、計算の一部を \"打ち消す\" ことができるため、効率よく微分計算ができるという仕組みである"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単純な算術ノードの実装\n",
    "\n",
    "計算グラフの単純なノードを実装していく\n",
    "\n",
    "ただし、型名は「ノード」ではなく、ニューラルネットワークの「層（レイヤ）」を意味するものとして `***Layer` という名前で実装することにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべてのレイヤの基底となる抽象型: 抽象レイヤ\n",
    "abstract type AbstractLayer end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加算ノード（加算レイヤ）\n",
    "まず、加算ノード $z = x + y$ について考える\n",
    "\n",
    "このノードの逆伝播（偏微分）は以下のようになる\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    "    \\frac{∂z}{∂x} = 1 \\\\\n",
    "    \\frac{∂z}{∂y} = 1\n",
    "\\end{array} $$\n",
    "\n",
    "従って、この計算グラフは以下のようになる\n",
    "\n",
    "![computational_graph_add.png](./img/computational_graph_add.png)\n",
    "\n",
    "上図のように、加算ノードの逆伝播は、上流の値がそのまま分岐して流れていく\n",
    "\n",
    "これを実装すると以下のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@note:\n",
    "    forward, backward は引数のレイヤに対して破壊的な変更を加える可能性がある\n",
    "    => forward!, backward! という関数名で実装しておく\n",
    "\"\"\"\n",
    "# 加算レイヤ\n",
    "mutable struct AddLayer <: AbstractLayer end\n",
    "\n",
    "# 加算レイヤ: 順伝播\n",
    "## x, y: 上流から流れてきる2値 => 下流に流す値\n",
    "forward!(layer::AddLayer, x::Float64, y::Float64)::Float64 = x + y\n",
    "\n",
    "# 加算レイヤ: 逆伝播\n",
    "## dout: 上流から流れてくる微分値 => 下流に流す2値\n",
    "backward!(layer::AddLayer, dout::Float64)::Tuple{Float64,Float64} = (dout * 1, dout * 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 乗算ノード（乗算レイヤ）\n",
    "同様に、乗算ノード $z = x \\times y$ について考えると、このノードの逆伝播（偏微分）は以下のようになる\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    "    \\frac{∂z}{∂x} = y \\\\\n",
    "    \\frac{∂z}{∂y} = x\n",
    "\\end{array} $$\n",
    " \n",
    "従って、この計算グラフは以下のようになる\n",
    "\n",
    "![computational_graph_mul.png](./img/computational_graph_mul.png)\n",
    "\n",
    "すなわち、乗算ノードの逆伝播では、上流から流れてきた微分値に対して、順伝播の \"ひっくり返した値\" を乗算して流す形になる\n",
    "\n",
    "これを実装すると以下のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 乗算レイヤ\n",
    "mutable struct MulLayer <: AbstractLayer\n",
    "    x::Float64\n",
    "    y::Float64\n",
    "end\n",
    "\n",
    "MulLayer() = MulLayer(0, 0)\n",
    "\n",
    "# 乗算レイヤ: 順伝播\n",
    "## x, y: 上流から流れてきる2値 => 下流に流す値\n",
    "forward!(layer::MulLayer, x::Float64, y::Float64)::Float64 = begin\n",
    "    # 順伝播時に値を保持しておく\n",
    "    layer.x = x\n",
    "    layer.y = y\n",
    "    x * y\n",
    "end\n",
    "\n",
    "# 乗算レイヤ: 逆伝播\n",
    "## dout: 上流から流れてくる微分値 => 下流に流す2値\n",
    "backward!(layer::MulLayer, dout::Float64)::Tuple{Float64,Float64} = (dout * layer.y, dout * layer.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで加算レイヤと乗算レイヤを実装できたため、これらを用いて少し複雑な計算グラフを実装する\n",
    "\n",
    "前述した「りんご2個とみかん3個の買い物」の計算グラフを以下に示す\n",
    "\n",
    "![computational_graph_backword.png](./img/computational_graph_backword.png)\n",
    "\n",
    "これを実装すると以下のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715.0000000000001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 変数\n",
    "apple = 100.0\n",
    "apple_num = 2.0\n",
    "orange = 150.0\n",
    "orange_num = 3.0\n",
    "tax = 1.1\n",
    "\n",
    "# レイヤ（ノード）\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 順伝播\n",
    "apple_price = forward!(mul_apple_layer, apple, apple_num)\n",
    "orange_price = forward!(mul_orange_layer, orange, orange_num)\n",
    "all_price = forward!(add_apple_orange_layer, apple_price, orange_price)\n",
    "price = forward!(mul_tax_layer, all_price, tax) # => 715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.00000000000001, 2.2, 165.0, 3.3000000000000003, 650.0\n"
     ]
    }
   ],
   "source": [
    "# 逆伝播\n",
    "d_price = 1.0\n",
    "d_all_price, d_tax = backward!(mul_tax_layer, d_price)\n",
    "d_apple_price, d_orange_price = backward!(add_apple_orange_layer, d_all_price)\n",
    "d_orange, d_orange_num = backward!(mul_orange_layer, d_orange_price)\n",
    "d_apple, d_apple_num = backward!(mul_apple_layer, d_apple_price)\n",
    "\n",
    "println(\"$d_apple_num, $d_apple, $d_orange_num, $d_orange, $d_tax\")\n",
    "# => d_apple_num: 110, d_apple: 2.2, d_orange_num: 165, d_orange: 3.3, d_tax: 650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 活性化関数レイヤの実装\n",
    "\n",
    "計算グラフの考え方をニューラルネットワークに適用していく\n",
    "\n",
    "### ReLUレイヤ\n",
    "ReLU活性化関数は以下の式で表される\n",
    "\n",
    "$$\n",
    "    y = \\begin{cases}\n",
    "        x & (x > 0) \\\\\n",
    "        0 & (x \\leq 0)\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "よって、微分は以下の式で表される\n",
    "\n",
    "$$\n",
    "    \\frac{∂y}{∂x} = \\begin{cases}\n",
    "        1 & (x > 0) \\\\\n",
    "        0 & (x \\leq 0)\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "従って、ReLU活性化関数を計算グラフで表すと以下のようになる\n",
    "\n",
    "![computational_graph_relu.png](./img/computational_graph_relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 3 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReLUレイヤ\n",
    "mutable struct ReluLayer <: AbstractLayer\n",
    "    mask::AbstractArray{Bool}\n",
    "    ReluLayer() = new()\n",
    "end\n",
    "\n",
    "# ReLUレイヤ: 順伝播\n",
    "forward!(layer::ReluLayer, x::AbstractArray{Float64})::AbstractArray{Float64} = begin\n",
    "    mask = layer.mask = (x .<= 0) # x <= 0 の要素をマスキング\n",
    "    out = copy(x) # 入力値をそのまま出力値にコピー\n",
    "    out[mask] .= zero(Float64) # x <= 0 でマスキングした要素それぞれに 0 代入\n",
    "    out\n",
    "end\n",
    "\n",
    "# ReLUレイヤ: 逆伝播\n",
    "backward!(layer::ReluLayer, dout::AbstractArray{Float64})::AbstractArray{Float64} = begin\n",
    "    # 基本的に上流から流れてきた微分値をそのまま下流へ\n",
    "    dout[layer.mask] .= zero(Float64) # x <= 0 でマスキングした要素それぞれに 0 代入\n",
    "    dout\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 1.0  0.0\n",
       " 0.0  3.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@test: ReLUレイヤ動作確認\n",
    "\"\"\"\n",
    "# ReLUレイヤ: 順伝播\n",
    "## [1.0 -0.5; -2.0 3.0] => [1.0 0.0; 0.0 3.0]\n",
    "relu = ReluLayer()\n",
    "forward!(relu, [1.0 -0.5; -2.0 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 BitArray{2}:\n",
       " 0  1\n",
       " 1  0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReLUレイヤ: マスク状態（x <= 0 のindex）\n",
    "## => [false true; true false]\n",
    "relu.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 1.0  0.0\n",
       " 0.0  1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReLUレイヤ: 逆伝播\n",
    "## [1.0 1.0; 1.0 1.0] => [1.0 0.0; 0.0 1.0]\n",
    "backward!(relu, [1.0 1.0; 1.0 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoidレイヤ\n",
    "シグモイド関数は以下の式で表される\n",
    "\n",
    "$$\n",
    "    y = \\frac{1}{1 + \\exp(-x)}\n",
    "$$\n",
    "\n",
    "これを計算グラフで表すと以下のようになる\n",
    "\n",
    "![computational_graph_sigmoid.png](./img/computational_graph_sigmoid.png)\n",
    "\n",
    "この計算グラフの逆伝播をステップごとに分解して考えると以下のようになっている\n",
    "\n",
    "1. 「／」ノードは $y=\\frac{1}{x}$ を表し、この微分は解析的に以下のように解くことができる\n",
    "    $$ \\begin{array}{ll}\n",
    "        \\frac{∂y}{∂x} &= -\\frac{1}{x^2} \\\\\n",
    "                        &= -y^2\n",
    "    \\end{array} $$\n",
    "2. 「＋」ノードは上流から流れてきた微分値をそのまま下流に流す（加算ノードの項参照）\n",
    "3. 「exp」ノードは $y=e^x$ を表し、この微分は解析的に以下のように解くことができる\n",
    "    $$\n",
    "        \\frac{∂y}{∂x} = e^x\n",
    "    $$\n",
    "4. 「×」ノードは、順伝播時の2値を \"ひっくり返して\" 乗算するため、ここでは -1 を乗算している\n",
    "\n",
    "ここで、$\\frac{∂L}{∂y}y^2 e^{-x}$ が順伝播の値 $x$, $y$ のみから計算可能であることに注目すると、上記計算グラフは、以下のように「sigmoid」ノードとしてグループ化することができる\n",
    "\n",
    "![computational_graph_sigmoid2.png](./img/computational_graph_sigmoid2.png)\n",
    "\n",
    "このようにグループ化することで途中の計算を省略することができるため、計算効率を向上させることができる\n",
    "\n",
    "さらに $\\frac{∂y}{∂x}y^2 e^{-x}$ は以下のように展開できるため、さらに整理できる\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    "    \\frac{∂y}{∂x}y^2 e^{-x} &= \\frac{∂y}{∂x}\\frac{1}{(1 + e^{-x})^2}e^{-x} & (∵ y = \\frac{1}{1 + e^{-x}}) \\\\\n",
    "    &= \\frac{∂y}{∂x}\\frac{1}{1 + e^{-x}}\\frac{e^{-x}}{1 + e^{-x}} \\\\\n",
    "    &= \\frac{∂y}{∂x}y(1-y)\n",
    "\\end{array} $$\n",
    "\n",
    "従って、最終的に計算グラフは以下のようにまとめることができる\n",
    "\n",
    "![computational_graph_sigmoid3.png](./img/computational_graph_sigmoid3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoiodレイヤ\n",
    "mutable struct SigmoidLayer <: AbstractLayer\n",
    "    out::AbstractArray{Float64} # Sigmoidレイヤが保持しておかなければならないのは順伝播時の出力値 y\n",
    "end\n",
    "\n",
    "# Sigmoiodレイヤ: 順伝播\n",
    "forward!(layer::SigmoidLayer, x::AbstractArray{Float64})::AbstractArray{Float64} =\n",
    "    layer.out = 1 ./ (1 .+ exp.(-x))\n",
    "\n",
    "# Sigmoiodレイヤ: 逆伝播\n",
    "backward!(layer::SigmoidLayer, dout::AbstractArray{Float64})::AbstractArray{Float64} =\n",
    "    dount .* layer.out .* (1 .- layer.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算グラフを使って途中計算を整理することによって、上記のように逆伝播（微分）を非常に簡単な計算に置き換えられるということは重要である\n",
    "\n",
    "### Affineレイヤ\n",
    "ニューラルネットワークの順伝播時で行われる計算は以下のようなものであった\n",
    "\n",
    "$$\n",
    "        出力行列 = 入力行列 \\cdot 重み行列 + バイアス行列\n",
    "$$\n",
    "\n",
    "ここで、行列の内積計算は、幾何学分野で**アフィン変換**と呼ばれる\n",
    "\n",
    "そのため、ニューラルネットワーク順伝播時の行列計算を行う層を**Affineレイヤ**として実装することにする\n",
    "\n",
    "まず、計算グラフを考える\n",
    "\n",
    "ここでは、バッチサイズを $N$, 入力値の数を 2, 重みを 2×3行列（入力数: 2, 出力数: 3）,  バイアスを 1×3行列（出力数: 3）として考えると、計算グラフは以下のようになる\n",
    "\n",
    "![computational_graph_affine.png](./img/computational_graph_affine.png)\n",
    "\n",
    "なお「dot」ノードは、実質的には「×」ノードと同一と考えて良いため、順伝播時の2入力値を \"ひっくり返して\" 乗算することで逆伝播の流れを作ることができる\n",
    "\n",
    "ただし、行列の値をひっくり返すということは、転置行列を作ることを意味する\n",
    "\n",
    "そのため、入力行列 $X$ の側における逆伝播は $\\frac{∂L}{∂Y} \\cdot W^T$ となり（$W^T$ は $W$ の転置行列）、重み行列 $W$ の側における逆伝播は $X^T \\cdot \\frac{∂L}{∂Y}$ となっている（$X^T$ は $X$ の転置行列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# バッチ対応Affineレイヤ\n",
    "mutable struct AffineLayer <: AbstractLayer\n",
    "    # AbstractArray型にすることでコンストラクタですべてのメンバの値を指定を不要にする\n",
    "    W::AbstractArray{Float64,2}\n",
    "    B::AbstractArray{Float64,2}\n",
    "    X::AbstractArray{Float64,2}\n",
    "    dW::AbstractArray{Float64,2}\n",
    "    dB::AbstractArray{Float64,2}\n",
    "    \n",
    "    # コンストラクタで重みとバイアスだけ指定できるように定義\n",
    "    function AffineLayer(W::AbstractArray{Float64,2}, B::AbstractArray{Float64,2})\n",
    "        layer = new()\n",
    "        layer.W = W\n",
    "        layer.B = B\n",
    "        layer\n",
    "    end\n",
    "end\n",
    "\n",
    "# Affineレイヤ: 順伝播\n",
    "forward!(layer::AffineLayer, x::Array{Float64,2})::Array{Float64,2} = begin\n",
    "    layer.X = x\n",
    "    x * layer.W .+ layer.B\n",
    "end\n",
    "\n",
    "# Affineレイヤ: 逆伝播\n",
    "backward!(layer::AffineLayer, dout::Array{Float64,2})::Array{Float64,2} = begin\n",
    "    dX = dout * layer.W'\n",
    "    layer.dW = layer.X' * dout\n",
    "    layer.dB = sum(dout, dims=1) # バイアスの逆伝播値: ∂L/∂Y の最初の軸に対する和\n",
    "    dX\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Array{Float64,2}:\n",
       "  1.0   2.0   3.0\n",
       " 11.0  12.0  13.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@test: Affineレイヤ動作確認\n",
    "\"\"\"\n",
    "# Affineレイヤ\n",
    "## 重み行列: 1×3（入力数: 1, 出力数: 3）\n",
    "## バイアス行列: 1×3（出力数: 3）\n",
    "layer = AffineLayer([5.0 5.0 5.0], [1.0 2.0 3.0])\n",
    "\n",
    "# 順伝播\n",
    "## 入力行列: 2×1（バッチサイズ: 2, 特徴量の数: 1）\n",
    "x = reshape([0.0; 2.0], 2, 1)\n",
    "forward!(layer, x) # => 2×3 [1 2 3; 11 12 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×1 Array{Float64,2}:\n",
       " 30.0\n",
       " 75.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 逆伝播\n",
    "## 入力微分値: 2×3行列（バッチサイズ: 2, Affineレイヤの順伝播出力数: 3）\n",
    "dY = [1.0 2.0 3.0; 4.0 5.0 6.0]\n",
    "backward!(layer, dY) # => 2×1 行列 X に戻る [30; 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Array{Float64,2}:\n",
       " 5.0  7.0  9.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 逆伝播後の微分バイアス\n",
    "layer.dB # => 1×3 [5 7 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmaxレイヤ\n",
    "Softmax活性化関数は以下の数式で表される\n",
    "\n",
    "$$\n",
    "    y_i = \\frac{e^{x_i}}{\\sum_{i=1}^N e^{x_i}}\n",
    "$$\n",
    "\n",
    "計算グラフで表すと以下のようになる\n",
    "\n",
    "![computational_graph_softmax.png](./img/computational_graph_softmax.png)\n",
    "\n",
    "続いて、①〜⑥の経路について逆伝播を考える\n",
    "\n",
    "1. $\\frac{∂L}{∂y_i}$\n",
    "2. 「×」ノードのため、順伝播時の相方を乗算する\n",
    "    $$\n",
    "        \\frac{∂L}{∂y_i} \\cdot e^{x_i}\n",
    "    $$\n",
    "3. 「／」ノードのため、上流値を2乗し符号反転する\n",
    "    - さらに各エッジの値を合計する\n",
    "    $$\n",
    "        \\sum_{i=1}^{N} -(\\frac{∂L}{∂y_i} \\cdot e^{x_i})^2\n",
    "    $$\n",
    "4. 「×」ノードのため、順伝播時の相方を乗算する\n",
    "    $$\n",
    "        \\frac{∂L}{∂y_i} \\cdot \\frac{1}{S} = \\frac{∂L}{∂y_i} \\cdot \\frac{1}{\\sum_{i=1}^{N}e^{x_i}}\n",
    "    $$\n",
    "5. 「＋」ノードのため、上流値をそのまま流す\n",
    "    $$\n",
    "        \\sum_{i=1}^{N} -(\\frac{∂L}{∂y_i} \\cdot e^{x_i})^2\n",
    "    $$\n",
    "6. 「exp」ノードのため、順伝播時の出力値を乗算する\n",
    "    $$\n",
    "        e^{x_i} \\cdot \\sum_{i=1}^{N} -(\\frac{∂L}{∂y_i} \\cdot e^{x_i})^2\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
