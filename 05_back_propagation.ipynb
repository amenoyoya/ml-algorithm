{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差逆伝播法\n",
    "\n",
    "## 数値微分の難点\n",
    "\n",
    "前回までで、ニューラルネットワークの基本的な学習方法である、確率的勾配降下法は実装できた\n",
    "\n",
    "このとき、勾配の計算は数値微分によって実装していた\n",
    "\n",
    "数値微分はシンプルに実装できて分かりやすいが、計算に時間がかかるという難点がある\n",
    "\n",
    "そこで、ここでは、重みパラメータの勾配計算を効率良く行う**誤差逆伝播法**を実装する\n",
    "\n",
    "\n",
    "## 計算グラフ\n",
    "\n",
    "誤差逆伝播法を視覚的に理解する方法として**計算グラフ（Computational graph）**というものがある\n",
    "\n",
    "計算グラフとは計算の過程をグラフによって表したものである\n",
    "\n",
    "ここで言うグラフとは、データ構造としてのグラフであり、複数のノードとエッジ（ノード間を結ぶ直線）によって表現されるものである\n",
    "\n",
    "### 計算グラフで計算問題を解く\n",
    "以下のような簡単な計算問題を、計算グラフを使って解いてみる\n",
    "\n",
    "**問:** スーパーで1個100円のりんごを2個買ったとき、支払う金額はいくらか？（消費税は10％とする）\n",
    "\n",
    "計算グラフは以下のように、ノードごとの計算結果が左から右へ伝わるように表現する\n",
    "\n",
    "![computational_graph01.png](./img/computational_graph01.png)\n",
    "\n",
    "上記では、「×2」や「×1.1」を一つの演算として○（ノード）でくくっているが、「×」のような演算子は単一のノードで表現するのが望ましい\n",
    "\n",
    "この場合、「2」と「1.1」は、それぞれ「りんごの個数」と「消費税」という変数として、ノードの外側に表記する\n",
    "\n",
    "![computational_graph02.png](./img/computational_graph02.png)\n",
    "\n",
    "次に、もう少し複雑な計算を計算グラフで解く\n",
    "\n",
    "**問:** スーパーでりんご（100円／個）を2個、みかん（150円／個）を3個買ったとき、支払う金額はいくらか？（消費税は10％とする）\n",
    "\n",
    "この問題を解くための計算グラフは以下のようになる\n",
    "\n",
    "![computational_graph03.png](./img/computational_graph03.png)\n",
    "\n",
    "このように、計算グラフを使って問題を解くには、\n",
    "\n",
    "1. 計算式に使われる要素を演算子と変数に分ける\n",
    "2. 演算子と変数をノードとしてエッジでつなぐ\n",
    "3. 計算グラフ上で計算を左から右へ進める\n",
    "\n",
    "という流れで作業する\n",
    "\n",
    "ここで、「計算を左から右へ進める」処理を**順伝播（forward propagation）**と呼び、ニューラルネットワークの**推論処理**に対応する\n",
    "\n",
    "逆に「計算を右から左へ戻る」処理を**逆伝播（back propagation）**と呼び、ニューラルネットワークの**学習処理**に対応する\n",
    "\n",
    "この逆伝播は、この先、微分を計算するにあたって重要な働きをする\n",
    "\n",
    "### 局所的計算と逆伝播による微分\n",
    "計算グラフを使う利点は大きく以下の2点がある\n",
    "\n",
    "- 「局所的な計算」を伝播することにより複雑な計算を行うことができる\n",
    "    - 各ノードは、全体の計算には関与せず「自分に関係する小さな範囲」の計算だけを行う（局所的計算）\n",
    "    - これにより計算を単純化することができる\n",
    "    - また、計算途中の結果をすべて、各ノードで保持することも可能\n",
    "- 計算グラフを逆伝播することで微分値を効率よく計算できる\n",
    "    - 順伝播が「局所的な計算」であるのと同様に、逆伝播は「局所的な微分」を表す\n",
    "    - これにより微分計算を単純化し、計算速度を向上させることができる\n",
    "\n",
    "例えば上記の問題について、りんごの値段が値上がりした場合、最終的な支払金額にどのように影響するか知りたいとする\n",
    "\n",
    "これは「りんごの値段に対する支払金額の微分」を求めることに相当する（りんごの値段を $x$, 支払金額を $L$ とした場合、$∂L/∂x$ を計算することに相当する）\n",
    "\n",
    "計算グラフの逆伝播によって、この問題を解くと以下のようになる\n",
    "\n",
    "![computational_graph04.png](./img/computational_graph04.png)\n",
    "\n",
    "上記のように、計算グラフを右から左へ計算することで「局所的な微分」を伝達することができる\n",
    "\n",
    "この結果から「りんごの値段に関する支払金額の微分」の値は 2.2 であると言える\n",
    "\n",
    "すなわち、りんごが1円値上がりしたら、最終的な支払金額は2.2円増えることを意味する（正確には、りんごの値段がある微小な値だけ増えたら、最終的な支払金額はその微小な値の2.2倍だけ増加することを意味する）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 連鎖律\n",
    "\n",
    "### 計算グラフの逆伝播\n",
    "計算グラフの逆伝播を一般化すると以下のように表すことができる\n",
    "\n",
    "![computational_graph05.png](./img/computational_graph05.png)\n",
    "\n",
    "ここで、上記計算グラフは $y = f(x)$ という計算を表現している\n",
    "\n",
    "逆伝播の計算手順は、信号 $E$ に対して、ノードの局所的な微分 $\\frac{∂y}{∂x}$ を乗算し、次のノードで伝達する、というものになっている\n",
    "\n",
    "これは、前述したりんごの支払金額計算で考えると分かりやすい\n",
    "\n",
    "```\n",
    "最初のノード: f(x) = 2x (x: 前のノードの出力値＝りんごの値段, 2: りんごの個数)\n",
    "最後のノード: g(x) = 1.1x (x: 前のノードの出力値, 1.1: 消費税倍率)\n",
    "\n",
    "最後のノードの逆伝播: 1 * g'(x) = 1 * 1.1 = 1.1\n",
    "最初のノードの逆伝播: 1.1 * f'(x) = 1.1 * 2 = 2.2\n",
    "```\n",
    "\n",
    "この計算により効率よく微分値を求めることができるだが、その理由は**連鎖律の原理**から説明できる\n",
    "\n",
    "### 連鎖律\n",
    "以下のような計算グラフを考える\n",
    "\n",
    "![computational_graph06.png](./img/computational_graph06.png)\n",
    "\n",
    "この計算を微分すると、以下のような逆伝播で表現される\n",
    "\n",
    "![computational_graph07.png](./img/computational_graph07.png)\n",
    "\n",
    "ここで、合成関数の定理より $\\frac{∂z}{∂z} \\frac{∂z}{∂y}$ の $∂z$ は \"打ち消し合い\"、$\\frac{∂z}{∂y}$ となる\n",
    "\n",
    "同様にして $\\frac{∂z}{∂z} \\frac{∂z}{∂y} \\frac{∂y}{∂x} = \\frac{∂z}{∂x}$ となる\n",
    "\n",
    "このような合成関数の微分の性質を**連鎖律**と呼ぶ\n",
    "\n",
    "連鎖律により、計算の一部を \"打ち消す\" ことができるため、効率よく微分計算ができるという仕組みである"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単純な算術ノードの実装\n",
    "\n",
    "計算グラフの単純なノードを実装していく\n",
    "\n",
    "ただし、型名は「ノード」ではなく、ニューラルネットワークの「層（レイヤ）」を意味するものとして `***Layer` という名前で実装することにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべてのレイヤの基底となる抽象型: 抽象レイヤ\n",
    "abstract type AbstractLayer end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加算ノード（加算レイヤ）\n",
    "まず、加算ノード $z = x + y$ について考える\n",
    "\n",
    "このノードの逆伝播（偏微分）は以下のようになる\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    "    \\frac{∂z}{∂x} = 1 \\\\\n",
    "    \\frac{∂z}{∂y} = 1\n",
    "\\end{array} $$\n",
    "\n",
    "従って、この計算グラフは以下のようになる\n",
    "\n",
    "![computational_graph_add.png](./img/computational_graph_add.png)\n",
    "\n",
    "上図のように、加算ノードの逆伝播は、上流の値がそのまま分岐して流れていく\n",
    "\n",
    "これを実装すると以下のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@note:\n",
    "    forward, backward は引数のレイヤに対して破壊的な変更を加える可能性がある\n",
    "    => forward!, backward! という関数名で実装しておく\n",
    "\"\"\"\n",
    "# 加算レイヤ\n",
    "mutable struct AddLayer <: AbstractLayer end\n",
    "\n",
    "# 加算レイヤ: 順伝播\n",
    "## x, y: 上流から流れてきる2値 => 下流に流す値\n",
    "forward!(layer::AddLayer, x::Float64, y::Float64)::Float64 = x + y\n",
    "\n",
    "# 加算レイヤ: 逆伝播\n",
    "## dout: 上流から流れてくる微分値 => 下流に流す2値\n",
    "backward!(layer::AddLayer, dout::Float64)::Tuple{Float64,Float64} = (dout * 1, dout * 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 乗算ノード（乗算レイヤ）\n",
    "同様に、乗算ノード $z = x \\times y$ について考えると、このノードの逆伝播（偏微分）は以下のようになる\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    "    \\frac{∂z}{∂x} = y \\\\\n",
    "    \\frac{∂z}{∂y} = x\n",
    "\\end{array} $$\n",
    " \n",
    "従って、この計算グラフは以下のようになる\n",
    "\n",
    "![computational_graph_mul.png](./img/computational_graph_mul.png)\n",
    "\n",
    "すなわち、乗算ノードの逆伝播では、上流から流れてきた微分値に対して、順伝播の \"ひっくり返した値\" を乗算して流す形になる\n",
    "\n",
    "これを実装すると以下のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 乗算レイヤ\n",
    "mutable struct MulLayer <: AbstractLayer\n",
    "    x::Float64\n",
    "    y::Float64\n",
    "end\n",
    "\n",
    "MulLayer() = MulLayer(0, 0)\n",
    "\n",
    "# 乗算レイヤ: 順伝播\n",
    "## x, y: 上流から流れてきる2値 => 下流に流す値\n",
    "forward!(layer::MulLayer, x::Float64, y::Float64)::Float64 = begin\n",
    "    # 順伝播時に値を保持しておく\n",
    "    layer.x = x\n",
    "    layer.y = y\n",
    "    x * y\n",
    "end\n",
    "\n",
    "# 乗算レイヤ: 逆伝播\n",
    "## dout: 上流から流れてくる微分値 => 下流に流す2値\n",
    "backward!(layer::MulLayer, dout::Float64)::Tuple{Float64,Float64} = (dout * layer.y, dout * layer.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで加算レイヤと乗算レイヤを実装できたため、これらを用いて少し複雑な計算グラフを実装する\n",
    "\n",
    "前述した「りんご2個とみかん3個の買い物」の計算グラフを以下に示す\n",
    "\n",
    "![computational_graph_backword.png](./img/computational_graph_backword.png)\n",
    "\n",
    "これを実装すると以下のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715.0000000000001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 変数\n",
    "apple = 100.0\n",
    "apple_num = 2.0\n",
    "orange = 150.0\n",
    "orange_num = 3.0\n",
    "tax = 1.1\n",
    "\n",
    "# レイヤ（ノード）\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 順伝播\n",
    "apple_price = forward!(mul_apple_layer, apple, apple_num)\n",
    "orange_price = forward!(mul_orange_layer, orange, orange_num)\n",
    "all_price = forward!(add_apple_orange_layer, apple_price, orange_price)\n",
    "price = forward!(mul_tax_layer, all_price, tax) # => 715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.00000000000001, 2.2, 165.0, 3.3000000000000003, 650.0\n"
     ]
    }
   ],
   "source": [
    "# 逆伝播\n",
    "d_price = 1.0\n",
    "d_all_price, d_tax = backward!(mul_tax_layer, d_price)\n",
    "d_apple_price, d_orange_price = backward!(add_apple_orange_layer, d_all_price)\n",
    "d_orange, d_orange_num = backward!(mul_orange_layer, d_orange_price)\n",
    "d_apple, d_apple_num = backward!(mul_apple_layer, d_apple_price)\n",
    "\n",
    "println(\"$d_apple_num, $d_apple, $d_orange_num, $d_orange, $d_tax\")\n",
    "# => d_apple_num: 110, d_apple: 2.2, d_orange_num: 165, d_orange: 3.3, d_tax: 650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 活性化関数レイヤの実装\n",
    "\n",
    "計算グラフの考え方をニューラルネットワークに適用していく\n",
    "\n",
    "### ReLUレイヤ\n",
    "ReLU活性化関数は以下の式で表される\n",
    "\n",
    "$$\n",
    "    y = \\begin{cases}\n",
    "        x & (x > 0) \\\\\n",
    "        0 & (x \\leq 0)\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "よって、微分は以下の式で表される\n",
    "\n",
    "$$\n",
    "    \\frac{∂y}{∂x} = \\begin{cases}\n",
    "        1 & (x > 0) \\\\\n",
    "        0 & (x \\leq 0)\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "従って、ReLU活性化関数を計算グラフで表すと以下のようになる\n",
    "\n",
    "![computational_graph_relu.png](./img/computational_graph_relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 3 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReLUレイヤ\n",
    "mutable struct ReluLayer <: AbstractLayer\n",
    "    mask::AbstractArray{Bool}\n",
    "    ReluLayer() = new()\n",
    "end\n",
    "\n",
    "# ReLUレイヤ: 順伝播\n",
    "forward!(layer::ReluLayer, x::AbstractArray{Float64})::AbstractArray{Float64} = begin\n",
    "    mask = layer.mask = (x .<= 0) # x <= 0 の要素をマスキング\n",
    "    out = copy(x) # 入力値をそのまま出力値にコピー\n",
    "    out[mask] .= zero(Float64) # x <= 0 でマスキングした要素それぞれに 0 代入\n",
    "    out\n",
    "end\n",
    "\n",
    "# ReLUレイヤ: 逆伝播\n",
    "backward!(layer::ReluLayer, dout::AbstractArray{Float64})::AbstractArray{Float64} = begin\n",
    "    # 基本的に上流から流れてきた微分値をそのまま下流へ\n",
    "    dout[layer.mask] .= zero(Float64) # x <= 0 でマスキングした要素それぞれに 0 代入\n",
    "    dout\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 1.0  0.0\n",
       " 0.0  3.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@test: ReLUレイヤ動作確認\n",
    "\"\"\"\n",
    "# ReLUレイヤ: 順伝播\n",
    "## [1.0 -0.5; -2.0 3.0] => [1.0 0.0; 0.0 3.0]\n",
    "layer = ReluLayer()\n",
    "forward!(layer, [1.0 -0.5; -2.0 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 BitArray{2}:\n",
       " 0  1\n",
       " 1  0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReLUレイヤ: マスク状態（x <= 0 のindex）\n",
    "## => [false true; true false]\n",
    "layer.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 1.0  0.0\n",
       " 0.0  1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReLUレイヤ: 逆伝播\n",
    "## [1.0 1.0; 1.0 1.0] => [1.0 0.0; 0.0 1.0]\n",
    "backward!(layer, [1.0 1.0; 1.0 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoidレイヤ\n",
    "シグモイド関数は以下の式で表される\n",
    "\n",
    "$$\n",
    "    y = \\frac{1}{1 + \\exp(-x)}\n",
    "$$\n",
    "\n",
    "これを計算グラフで表すと以下のようになる\n",
    "\n",
    "![computational_graph_sigmoid.png](./img/computational_graph_sigmoid.png)\n",
    "\n",
    "この計算グラフの逆伝播をステップごとに分解して考えると以下のようになっている\n",
    "\n",
    "1. 「／」ノードは $y=\\frac{1}{x}$ を表し、この微分は解析的に以下のように解くことができる\n",
    "    $$ \\begin{array}{ll}\n",
    "        \\frac{∂y}{∂x} &= -\\frac{1}{x^2} \\\\\n",
    "                        &= -y^2\n",
    "    \\end{array} $$\n",
    "2. 「＋」ノードは上流から流れてきた微分値をそのまま下流に流す（加算ノードの項参照）\n",
    "3. 「exp」ノードは $y=e^x$ を表し、この微分は解析的に以下のように解くことができる\n",
    "    $$\n",
    "        \\frac{∂y}{∂x} = e^x\n",
    "    $$\n",
    "4. 「×」ノードは、順伝播時の2値を \"ひっくり返して\" 乗算するため、ここでは -1 を乗算している\n",
    "\n",
    "ここで、$\\frac{∂L}{∂y}y^2 e^{-x}$ が順伝播の値 $x$, $y$ のみから計算可能であることに注目すると、上記計算グラフは、以下のように「sigmoid」ノードとしてグループ化することができる\n",
    "\n",
    "![computational_graph_sigmoid2.png](./img/computational_graph_sigmoid2.png)\n",
    "\n",
    "このようにグループ化することで途中の計算を省略することができるため、計算効率を向上させることができる\n",
    "\n",
    "さらに $\\frac{∂y}{∂x}y^2 e^{-x}$ は以下のように展開できるため、さらに整理できる\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    "    \\frac{∂y}{∂x}y^2 e^{-x} &= \\frac{∂y}{∂x}\\frac{1}{(1 + e^{-x})^2}e^{-x} & (∵ y = \\frac{1}{1 + e^{-x}}) \\\\\n",
    "    &= \\frac{∂y}{∂x}\\frac{1}{1 + e^{-x}}\\frac{e^{-x}}{1 + e^{-x}} \\\\\n",
    "    &= \\frac{∂y}{∂x}y(1-y)\n",
    "\\end{array} $$\n",
    "\n",
    "従って、最終的に計算グラフは以下のようにまとめることができる\n",
    "\n",
    "![computational_graph_sigmoid3.png](./img/computational_graph_sigmoid3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoiodレイヤ\n",
    "mutable struct SigmoidLayer <: AbstractLayer\n",
    "    out::AbstractArray{Float64} # Sigmoidレイヤが保持しておかなければならないのは順伝播時の出力値 y\n",
    "end\n",
    "\n",
    "# Sigmoiodレイヤ: 順伝播\n",
    "forward!(layer::SigmoidLayer, x::AbstractArray{Float64})::AbstractArray{Float64} =\n",
    "    layer.out = 1 ./ (1 .+ exp.(-x))\n",
    "\n",
    "# Sigmoiodレイヤ: 逆伝播\n",
    "backward!(layer::SigmoidLayer, dout::AbstractArray{Float64})::AbstractArray{Float64} =\n",
    "    dount .* layer.out .* (1 .- layer.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算グラフを使って途中計算を整理することによって、上記のように逆伝播（微分）を非常に簡単な計算に置き換えられるということは重要である\n",
    "\n",
    "### Affineレイヤ\n",
    "ニューラルネットワークの順伝播時で行われる計算は以下のようなものであった\n",
    "\n",
    "$$\n",
    "        出力行列 = 入力行列 \\cdot 重み行列 + バイアス行列\n",
    "$$\n",
    "\n",
    "ここで、行列の内積計算は、幾何学分野で**アフィン変換**と呼ばれる\n",
    "\n",
    "そのため、ニューラルネットワーク順伝播時の行列計算を行う層を**Affineレイヤ**として実装することにする\n",
    "\n",
    "まず、計算グラフを考える\n",
    "\n",
    "ここでは、バッチサイズを $N$, 入力値の数を 2, 重みを 2×3行列（入力数: 2, 出力数: 3）,  バイアスを 1×3行列（出力数: 3）として考えると、計算グラフは以下のようになる\n",
    "\n",
    "![computational_graph_affine.png](./img/computational_graph_affine.png)\n",
    "\n",
    "なお「dot」ノードは、実質的には「×」ノードと同一と考えて良いため、順伝播時の2入力値を \"ひっくり返して\" 乗算することで逆伝播の流れを作ることができる\n",
    "\n",
    "ただし、行列の値をひっくり返すということは、転置行列を作ることを意味する\n",
    "\n",
    "そのため、入力行列 $X$ の側における逆伝播は $\\frac{∂L}{∂Y} \\cdot W^T$ となり（$W^T$ は $W$ の転置行列）、重み行列 $W$ の側における逆伝播は $X^T \\cdot \\frac{∂L}{∂Y}$ となっている（$X^T$ は $X$ の転置行列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 5 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# バッチ対応Affineレイヤ\n",
    "mutable struct AffineLayer <: AbstractLayer\n",
    "    # AbstractArray型にすることでコンストラクタですべてのメンバの値を指定を不要にする\n",
    "    W::AbstractArray{Float64,2}\n",
    "    B::AbstractArray{Float64,2}\n",
    "    X::AbstractArray{Float64,2}\n",
    "    dW::AbstractArray{Float64,2}\n",
    "    dB::AbstractArray{Float64,2}\n",
    "    \n",
    "    # コンストラクタで重みとバイアスだけ指定できるように定義\n",
    "    function AffineLayer(W::AbstractArray{Float64,2}, B::AbstractArray{Float64,2})\n",
    "        layer = new()\n",
    "        layer.W = W\n",
    "        layer.B = B\n",
    "        layer\n",
    "    end\n",
    "end\n",
    "\n",
    "# Affineレイヤ: 順伝播\n",
    "forward!(layer::AffineLayer, x::Array{Float64,2})::Array{Float64,2} = begin\n",
    "    layer.X = x\n",
    "    x * layer.W .+ layer.B\n",
    "end\n",
    "\n",
    "# Affineレイヤ: 逆伝播\n",
    "backward!(layer::AffineLayer, dout::Array{Float64,2})::Array{Float64,2} = begin\n",
    "    dX = dout * layer.W'\n",
    "    layer.dW = layer.X' * dout\n",
    "    layer.dB = sum(dout, dims=1) # バイアスの逆伝播値: ∂L/∂Y の最初の軸に対する和\n",
    "    dX\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Array{Float64,2}:\n",
       "  1.0   2.0   3.0\n",
       " 11.0  12.0  13.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@test: Affineレイヤ動作確認\n",
    "\"\"\"\n",
    "# Affineレイヤ\n",
    "## 重み行列: 1×3（入力数: 1, 出力数: 3）\n",
    "## バイアス行列: 1×3（出力数: 3）\n",
    "layer = AffineLayer([5.0 5.0 5.0], [1.0 2.0 3.0])\n",
    "\n",
    "# 順伝播\n",
    "## 入力行列: 2×1（バッチサイズ: 2, 特徴量の数: 1）\n",
    "x = reshape([0.0; 2.0], 2, 1)\n",
    "forward!(layer, x) # => 2×3 [1 2 3; 11 12 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×1 Array{Float64,2}:\n",
       " 30.0\n",
       " 75.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 逆伝播\n",
    "## 入力微分値: 2×3行列（バッチサイズ: 2, Affineレイヤの順伝播出力数: 3）\n",
    "dY = [1.0 2.0 3.0; 4.0 5.0 6.0]\n",
    "backward!(layer, dY) # => 2×1 行列 X に戻る [30; 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Array{Float64,2}:\n",
       " 5.0  7.0  9.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 逆伝播後の微分バイアス\n",
    "layer.dB # => 1×3 [5 7 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmaxレイヤ\n",
    "Softmax活性化関数は以下の数式で表される\n",
    "\n",
    "$$\n",
    "    y_i = \\frac{e^{x_i}}{\\sum_{i=1}^N e^{x_i}}\n",
    "$$\n",
    "\n",
    "計算グラフで表すと以下のようになる\n",
    "\n",
    "![computational_graph_softmax.png](./img/computational_graph_softmax.png)\n",
    "\n",
    "続いて、①〜⑥の経路について逆伝播を考える\n",
    "\n",
    "1. $\\frac{∂L}{∂y_i}$\n",
    "2. 「×」ノードのため、順伝播時の相方を乗算する\n",
    "    $$\n",
    "        \\frac{∂L}{∂y_i} \\cdot e^{x_i}\n",
    "    $$\n",
    "3. 「／」ノードのため、順伝播時の出力値を2乗し符号反転したものを乗算する\n",
    "    - 更にそれらを合計する\n",
    "    $$ \\begin{array}{ll}\n",
    "        -\\sum_{i=1}{N}\\frac{∂L}{∂y_i} \\cdot e^{x_i} \\cdot \\frac{1}{S^2} \\\\\n",
    "        = -\\sum_{i=1}^{N}\\frac{∂L}{∂y_i} \\cdot \\frac{y_i}{S} &(∵ y_i = \\frac{e^{x_i}}{S})\n",
    "    \\end{array} $$\n",
    "4. 「×」ノードのため、順伝播時の相方を乗算する\n",
    "    $$\n",
    "        \\frac{∂L}{∂y_i} \\cdot \\frac{1}{S}\n",
    "    $$\n",
    "5. 「＋」ノードのため、上流値をそのまま流す\n",
    "    $$\n",
    "        -\\sum_{i=1}^{N}\\frac{∂L}{∂y_i} \\cdot \\frac{y_i}{S}\n",
    "    $$\n",
    "6. 「exp」ノードのため、順伝播時の出力値を乗算する\n",
    "    - 今回の場合、上流値が④と⑤の2つあるため、これらの合計値に順伝播時の出力値を乗算することになる\n",
    "    $$ \\begin{array}{ll}\n",
    "        (\\frac{∂L}{∂y_i} \\cdot \\frac{1}{S} - \\sum_{j=1}^{N}\\frac{∂L}{∂y_j} \\cdot \\frac{y_j}{S}) \\cdot e^{x_i} \\\\\n",
    "        = \\frac{1}{S}(\\frac{∂L}{∂y_i} - \\sum_{j=1}^{N}\\frac{∂L}{∂y_j}y_j) \\cdot e^{x_i} \\\\\n",
    "        = y_i(\\frac{∂L}{∂y_i} - \\sum_{j=1}^{N}\\frac{∂L}{∂y_j}y_j) &(∵ y_i = \\frac{e^{x_i}}{S})\n",
    "    \\end{array} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmaxレイヤ＋交差エントロピー誤差レイヤ\n",
    "上記のようにSoftmaxレイヤの逆伝播計算は少々複雑になってしまっている\n",
    "\n",
    "しかし、交差エントロピー誤差レイヤを組み合わせることで非常に簡単な式に展開することができる\n",
    "\n",
    "ここで改めて、交差エントロピー誤差の式を掲載する\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    "    E = - \\sum_{k} t_k \\times \\ln{y_k}\n",
    "    \\\\\n",
    "    k: データの次元数 \\\\\n",
    "    y_k: ニューラルネットワークの出力（予測値） \\\\\n",
    "    t_k: 教師データ（正解値）\n",
    "    \\ln: 底がe(自然定数)の対数\n",
    "\\end{array} $$\n",
    "\n",
    "この計算グラフは以下のようになる\n",
    "\n",
    "![computational_graph_entropy.png](./img/computational_graph_entropy.png)\n",
    "\n",
    "このとき、交差エントロピー誤差の入力値 $y_i$ が、Softmaxレイヤの出力値 $y_i = \\frac{e^{x_i}}{S}$ であるとする（Softmaxレイヤと交差エントロピー誤差レイヤが連続的に接続されているとする）\n",
    "\n",
    "この場合、計算としては、Softmaxレイヤの逆伝播の出力値 $y_i(\\frac{∂L}{∂y_i} - \\sum_{j=1}^{N}\\frac{∂L}{∂y_j}y_j)$ に対して、以下のような置換処理を施せば良い\n",
    "\n",
    "$$\n",
    "    \\frac{∂L}{∂y_i} → -(\\frac{t_i}{y_i})\\frac{∂E}{∂y_i}\n",
    "$$\n",
    "\n",
    "したがって、Softmaxレイヤの逆伝播の出力値は以下のように展開できる\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    "    y_i(\\frac{∂L}{∂y_i} - \\sum_{j=1}^{N}\\frac{∂L}{∂y_j}y_j) \\\\\n",
    "    = y_i(-(\\frac{t_i}{y_i})\\frac{∂E}{∂y_i} - \\sum_{j=1}^{N}-(y_j\\frac{t_j}{y_j})\\frac{∂E}{∂y_j}) \\\\\n",
    "    = -t_i\\frac{∂E}{∂y_i} + y_i\\sum_{j=1}^{N}t_j\\frac{∂E}{∂y_j} \\\\\n",
    "    = -t_i\\frac{∂E}{∂y_i} + y_i\\frac{∂E}{∂y_i} &(∵ t: one-hot表現のため正解データのみ 1 で、それ以外のデータが 0 となる) \\\\\n",
    "    = (y_i - t_i)\\cdot \\frac{∂E}{∂y_i}\n",
    "\\end{array} $$\n",
    "\n",
    "以上より、Softmax＋交差エントロピー誤差レイヤは以下のように表現できる\n",
    "\n",
    "![computational_graph_softmax-entropy.png](./img/computational_graph_softmax-entropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 7 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax, cross_entropy_error 関数等読み込み\n",
    "include(\"./lib/Functions.jl\")\n",
    "\n",
    "# Softmax with Cross entropy error Layer\n",
    "mutable struct SoftmaxWithLossLayer <: AbstractLayer\n",
    "    loss::Float64 # 最終結果（誤差）\n",
    "    y::AbstractArray{Float64} # SoftmaxレイヤからCrossEntropyErrorレイヤへのデータ\n",
    "    t::AbstractArray{Float64} # 教師データ\n",
    "    SoftmaxWithLossLayer() = new()\n",
    "end\n",
    "\n",
    "# 順伝播関数\n",
    "## y: 予測結果行列, t: 教師データ行列 => loss: 誤差\n",
    "forward!(layer::SoftmaxWithLossLayer, y::Array{Float64,2}, t::Array{Float64,2})::Float64 = begin\n",
    "    layer.t = t\n",
    "    y = layer.y = softmax(y)\n",
    "    layer.loss = cross_entropy_error(y, t)\n",
    "end\n",
    "\n",
    "# 逆伝播関数\n",
    "## 教師データは 1×N 行列である想定であるため、バッチサイズで除算する必要あり\n",
    "backward!(layer::SoftmaxWithLossLayer, dout::Float64=1.0)::Array{Float64,2} = (layer.y .- layer.t) .* dout ./ size(layer.y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差逆伝播法によるニューラルネットワーク\n",
    "\n",
    "これまでに実装したレイヤを組み合わせることで、誤差逆伝播法に対応したニューラルネットワークを実装する\n",
    "\n",
    "ここでは、以下のような二層ニューラルネットワークを構築する\n",
    "\n",
    "- 入力層:\n",
    "    - 手書き数字の画像データ（サイズ: 28x28）\n",
    "    - ニューロン数: 28 * 28 = 784\n",
    "    - 各ニューロンの入力値は 0.0〜1.0 の実数型である必要がある\n",
    "- 中間層（隠れ層）1: `Affine`レイヤ => `ReLU`活性化レイヤ\n",
    "    - ニューロン数: 100\n",
    "- 出力層: `Affine`レイヤ => `SoftmaxWithLoss`誤差伝播レイヤ\n",
    "    - ニューロン数: 10（0〜9の数字クラスに分類するため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss! (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 二層ニューラルネットワーク\n",
    "mutable struct TwoLayerNet\n",
    "    affine_layer1::AffineLayer\n",
    "    relu_layer::ReluLayer\n",
    "    affine_layer2::AffineLayer\n",
    "    softmax_layer::SoftmaxWithLossLayer\n",
    "end\n",
    "\n",
    "TwoLayerNet(input_size::Int, hidden_size::Int, output_size::Int, weight_init_std::Float64=0.01) = begin\n",
    "    W1 = weight_init_std .* rand(Float64, input_size, hidden_size)\n",
    "    b1 = zeros(Float64, 1, hidden_size)\n",
    "    W2 = weight_init_std .* rand(Float64, hidden_size, output_size)\n",
    "    b2 = zeros(Float64, 1, output_size)\n",
    "    a1lyr = AffineLayer(W1, b1)\n",
    "    relu1lyr = ReluLayer()\n",
    "    a2lyr = AffineLayer(W2, b2)\n",
    "    softmaxlyr = SoftmaxWithLossLayer()\n",
    "    # TwoLayerNet(W1, b1, W2, b2, a1lyr, relu1lyr, a2lyr, softmaxlyr)\n",
    "    TwoLayerNet(a1lyr, relu1lyr, a2lyr, softmaxlyr)\n",
    "end\n",
    "\n",
    "# 二層ニューラルネットワーク: 順伝播（分類まで）\n",
    "predict!(net::TwoLayerNet, x::Array{Float64,2})::Array{Float64,2} = begin\n",
    "    a1 = forward!(net.affine_layer1, x)\n",
    "    z1 = forward!(net.relu_layer, a1)\n",
    "    a2 = forward!(net.affine_layer2, z1)\n",
    "    # 本来なら最後にsoftmax関数を噛ませるが\n",
    "    ## あえて確率を求めなくても a2 の大小で分類できるため計算を省略する\n",
    "    # softmax(a2)\n",
    "    a2\n",
    "end\n",
    "\n",
    "# 二層ニューラルネットワーク: 順伝播（誤差の算出まで）\n",
    "loss!(net::TwoLayerNet, x::Array{Float64,2}, t::Array{Float64,2})::Float64 = forward!(net.softmax_layer, predict!(net, x), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "applygradient! (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 誤差逆伝播: 更新用構造体\n",
    "struct TwoLayerNetGrads\n",
    "    W1::Array{Float64,2}\n",
    "    b1::Array{Float64,2}\n",
    "    W2::Array{Float64,2}\n",
    "    b2::Array{Float64,2}\n",
    "end\n",
    "\n",
    "# 逆伝播による微分計算\n",
    "gradient!(net::TwoLayerNet, x::Array{Float64,2}, t::Array{Float64,2}) = begin\n",
    "    # forward\n",
    "    loss!(net, x, t)\n",
    "    # backward\n",
    "    dout = one(Float64)\n",
    "    dz2 = backward!(net.softmax_layer, dout)\n",
    "    da2 = backward!(net.affine_layer2, dz2)\n",
    "    dz1 = backward!(net.relu_layer, da2)\n",
    "    da1 = backward!(net.affine_layer1, dz1)\n",
    "    TwoLayerNetGrads(net.affine_layer1.dW, net.affine_layer1.dB, net.affine_layer2.dW, net.affine_layer2.dB)\n",
    "end\n",
    "\n",
    "# パラメータ更新\n",
    "applygradient!(net::TwoLayerNet, grads::TwoLayerNetGrads, learning_rate::Float64) = begin\n",
    "    net.affine_layer1.W -= learning_rate .* grads.W1\n",
    "    net.affine_layer1.b -= learning_rate .* grads.b1\n",
    "    net.affine_layer2.W -= learning_rate .* grads.W2\n",
    "    net.affine_layer2.b -= learning_rate .* grads.b2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×10 Array{Float64,2}:\n",
       " 0.279128  0.282546  0.263375  0.262549  …  0.281995  0.251701  0.272658\n",
       " 0.312155  0.316891  0.293328  0.293775     0.3152    0.282885  0.30593 \n",
       " 0.196455  0.19848   0.183418  0.185771     0.197871  0.177133  0.192631"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@test: 誤差逆伝播法の勾配確認\n",
    "\"\"\"\n",
    "# MLDatasetsパッケージのMNISTSデータセットを使う\n",
    "using MLDatasets\n",
    "\n",
    "# 訓練用画像データと教師データをロード\n",
    "## train_x: 特徴量＝<画像データ｜28x28 グレースケール画像 60,000枚>{28x28x60000 Array{UInt8, 3}}\n",
    "## train_y: 目的変数＝<数値クラス｜[0..9]の数値 60,000個>{60000 Array{Int, 1}}\n",
    "train_x, train_y = MNIST.traindata()\n",
    "\n",
    "# 学習データをニューラルネットワーク用に前処理\n",
    "train_x = Array{Float64,3}(reshape(train_x, 1, 28*28, :)) # 1 x 784 x 60000 Array{Float64,3}\n",
    "train_x = permutedims(train_x, [3, 2, 1]) # 60000 x 784 x 1 Array{Float64,3}\n",
    "train_x = Array{Float64,2}(reshape(train_x, :, 784)) # 60000 x 784 Array{Float64,2}\n",
    "\n",
    "# 教師データをone-hot-vector形式に変換\n",
    "train_y = hcat([[i-1 == y ? 1.0 : 0.0 for i in 1:10] for y in train_y]...)' # 60000 x 10 Array{Float64,2}\n",
    "\n",
    "# 損失関数の履歴\n",
    "train_loss_list = []\n",
    "\n",
    "# ハイパーパラメータ\n",
    "batch_size = 3 # ミニバッチサイズ\n",
    "learning_rate = 0.1 # 学習率\n",
    "\n",
    "# 二層ニューラルネットワーク\n",
    "## 入力サイズ: 28×28 = 784, 隠れ層: 100ニューロン, 出力層: 10ニューロン\n",
    "net = TwoLayerNet(784, 100, 10)\n",
    "\n",
    "# 推論実行\n",
    "y = predict!(net, train_x[1:batch_size, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3030785385600256"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 推論＋誤差計算\n",
    "loss!(net, train_x[1:batch_size, :], train_y[1:batch_size, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000591 seconds (93 allocations: 679.344 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TwoLayerNetGrads([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.00077737345663737 -0.001977886945119239 … -0.001668302902350375 -0.0011535356919118876], [-0.13678417111302799 0.05067383135959191 … 0.04920380756923794 0.050210175253276305; -0.13830899689716822 0.0493173160635406 … 0.0478788661909025 0.04886284185579607; … ; -0.13850412657461555 0.05208456230139983 … 0.05057424624030005 0.051608216296411394; -0.15131815959821673 0.05352869251640413 … 0.051964950890714554 0.053034376858999324], [-0.23255673423652945 0.10111931864852024 … 0.09825542734012513 0.10022313018543082])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 逆伝播による微分\n",
    "grads = @time gradient!(net, train_x[1:batch_size, :], train_y[1:batch_size, :])\n",
    "\n",
    "## => 前回、数値微分で行った場合は同一の計算に約66秒かかっていたが\n",
    "### 今回の逆伝播による微分は 0.001秒未満の時間で計算できている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
