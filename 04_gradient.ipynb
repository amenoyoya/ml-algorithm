{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワーク学習処理\n",
    "\n",
    "## 復習｜勾配計算\n",
    "\n",
    "まずは、シンプルなニューラルネットワークを構築し、正しく勾配計算されるか確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Array{Float64,2}:\n",
       " 0.2  0.2  -0.4\n",
       " 0.3  0.3  -0.6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@test: 参考書との比較（正しく勾配計算されるか確認）\n",
    "\"\"\"\n",
    "# ニューラルネットワーク実装読み込み\n",
    "include(\"./lib/Neuron.jl\")\n",
    "include(\"./lib/Functions.jl\")\n",
    "\n",
    "# 数値微分による勾配計算\n",
    "## 関数(Array{Float64,2})::Float64, 入力値::Array{Float64,2} -> 勾配::Array{Float64,2}\n",
    "numeric_gradient(f, x::Array{Float64,2})::Array{Float64,2} = begin\n",
    "    h = 1e-4 # 10^(-4)\n",
    "    grad = Array{Float64, 2}(undef, size(x, 1), size(x, 2)) # xと同じ次元の行列を生成\n",
    "    # 各変数ごとの数値微分を行列にまとめる\n",
    "    for row in 1:size(x, 1), col in 1:size(x, 2)\n",
    "        # 指定indexの変数に対する中心差分を求める\n",
    "        org = x[row, col]\n",
    "        x[row, col] = org + h\n",
    "        f1 = f(x) # f([..., x[row, col] + h, ...]) -> Float64\n",
    "        x[row, col] = org - h\n",
    "        f2 = f(x) # f([..., x[row, col] - h, ...]) -> Float64\n",
    "        grad[row, col] = (f1 - f2) / 2h # (row, col)番目の変数に対する数値微分\n",
    "        x[row, col] = org # x[i]の値をもとに戻す\n",
    "    end\n",
    "    return grad\n",
    "end\n",
    "    \n",
    "# シンプルなニューラルネットワーク\n",
    "SimpleNet() = Network(1,\n",
    "    [\n",
    "        zeros(1, 3) # 1 x 3 Array{Float64,2} bias_1 [0 0 0]\n",
    "    ],\n",
    "    [\n",
    "        zeros(2, 3) # 2 x 3 Array{Float64,2} weight_1 [0 0 0; 0 0 0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 活性関数を使わず、ソフトマックス関数で出力するだけの推論処理\n",
    "predict(net::Network, x::Array{Float64,2})::Array{Float64,2} = softmax(x * net.w[1] + net.b[1])\n",
    "\n",
    "# 損失関数: 交差エントロピー誤差\n",
    "loss(net::Network, x::Array{Float64,2}, t::Array{Float64,2})::Float64 = cross_entropy_error(predict(net, x), t)\n",
    "\n",
    "x = [0.6 0.9]\n",
    "t = [0.0 0.0 1.0]\n",
    "\n",
    "net = SimpleNet()\n",
    "\n",
    "# 勾配計算\n",
    "## 2 x 3 Array{Float64,2} [0.2 0.2 -0.4; 0.3 0.3 -0.6] になればOK\n",
    "dW = numeric_gradient(w->loss(net, x, t), net.w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習アルゴリズムの実装\n",
    "\n",
    "これまでに実装した「損失関数」「ミニバッチ」「勾配」「勾配降下法」をまとめることで、ニューラルネットワークの学習アルゴリズムを実装することができる\n",
    "\n",
    "### 確率的勾配降下法\n",
    "ニューラルネットワークの学習手順は以下のようなものが基本となる\n",
    "\n",
    "1. ミニバッチ\n",
    "    - 訓練データからランダムに一部のデータを選び出す（ミニバッチ）\n",
    "    - 一回の学習においては、このミニバッチの損失関数の値を減少させることを目的とする\n",
    "2. 勾配\n",
    "    - ミニバッチの損失関数を減らすために、各重みパラメータの勾配を算出する\n",
    "    - 勾配は、損失関数の値を最も減らす方向を示す\n",
    "3. パラメータの更新\n",
    "    - 重みパラメータを勾配方向に微小量だけ更新する\n",
    "4. 1.に戻って同様の手順を繰り返す\n",
    "\n",
    "ここで、使用する訓練データをミニバッチとして無作為に選び出していることから、このような学習方法を**確率的勾配降下法**（Stochastic Gradient Descent）と呼ぶ\n",
    "\n",
    "ディープラーニングの多くのフレームワークでは、確率的勾配降下法の頭文字をとって**SGD**という名前の関数で実装されているのが一般的である\n",
    "\n",
    "### 2層ニューラルネットワークの実装\n",
    "今回は、手書き数字の学習を行うためのニューラルネットワークとして、2層ニューラルネットワーク（隠れ層1つのニューラルネットワーク）を実装することにする\n",
    "\n",
    "ネットワーク設計は以下の通りとする\n",
    "\n",
    "- 入力層:\n",
    "    - 手書き数字の画像データ（サイズ: 28x28）\n",
    "    - ニューロン数: 28 * 28 = 784\n",
    "    - 各ニューロンの入力値は 0.0〜1.0 の実数型である必要がある\n",
    "- 中間層（隠れ層）1:\n",
    "    - ニューロン数: 100\n",
    "    - 活性化関数: シグモイド関数\n",
    "- 出力層:\n",
    "    - ニューロン数: 10（0〜9の数字クラスに分類するため）\n",
    "    - 活性化関数: ソフトマックス関数\n",
    "    - 損失関数: 交差エントロピー関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Array{Float64,1}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " ⋮  \n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "2層ニューラルネットワークによる手書き数字の学習\n",
    "\"\"\"\n",
    "# Network構造体を継承して2層ニューラルネットワーク実装\n",
    "TwoLayerNetwork(weight_init_std::Float64=0.01) = Network(2,\n",
    "    [\n",
    "        zeros(Float64, 1, 100), # 1x100-Array{Float64,2} バイアス_1: 0行列\n",
    "        zeros(Float64, 1, 10),  # 1x10-Array{Float64,2} バイアス_2: 0行列\n",
    "    ],\n",
    "    [\n",
    "        rand(UInt8, 784, 100) * weight_init_std, # 784x100-Array{Float64,2} 重み_1: 任意整数 * weight_init_std の乱数行列\n",
    "        rand(UInt8, 100, 10) * weight_init_std, # 100x10-Array{Float64,2} 重み_2: 任意整数 * weight_init_std の乱数行列\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 推論処理\n",
    "## Network構造体, 入力信号 -> 出力信号 y\n",
    "predict(network::Network, x::Array{Float64,2})::Array{Float64,2} = predict(network, sigmoid, softmax, x)\n",
    "\n",
    "# 推論処理＋損失関数\n",
    "## Network構造体, 入力信号, 教師データ -> 交差エントロピー誤差\n",
    "loss(network::Network, x::Array{Float64,2}, t::Array{Float64,2})::Float64 = cross_entropy_error(predict(network, x), t)\n",
    "\n",
    "# 各パラメータの勾配計算\n",
    "## Network構造体, 入力信号, 教師データ -> 各パラメータの勾配行列をまとめた辞書\n",
    "numeric_gradient(network::Network, x::Array{Float64,2}, t::Array{Float64,2})::Dict{AbstractString, Array{Float64,2}} = begin\n",
    "    loss_func = w -> loss(network, x, t)\n",
    "    Dict(\n",
    "        \"B1\" => numeric_gradient(loss_func, network.b[1]),\n",
    "        \"B2\" => numeric_gradient(loss_func, network.b[2]),\n",
    "        \"W1\" => numeric_gradient(loss_func, network.w[1]),\n",
    "        \"W2\" => numeric_gradient(loss_func, network.w[2]),\n",
    "    )\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "@test: 2層ニューラルネットワークの推論処理\n",
    "\"\"\"\n",
    "# ダミー入力データ: 0.0〜1.0 の784サイズデータ 100枚分\n",
    "x = rand(Float64, 100, 784)\n",
    "\n",
    "# 推論実行\n",
    "net = TwoLayerNetwork()\n",
    "y = predict(net, x)\n",
    "\n",
    "# 各行ごとに列の合計値が1になっているか確認（softmax関数の特性の確認）\n",
    "[sum(y[row, :]) for row in 1:size(y, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 75.332291 seconds (148.44 M allocations: 107.002 GiB, 8.60% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{AbstractString,Array{Float64,2}} with 4 entries:\n",
       "  \"W2\" => [-0.150111 0.516357 … -4.31865e-6 -0.0843425; -0.150111 0.516357 … -4…\n",
       "  \"B2\" => [-0.150111 0.516357 … -4.31865e-6 -0.0843425]\n",
       "  \"B1\" => [0.0 0.0 … 0.0 0.0]\n",
       "  \"W1\" => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@test: 2層ニューラルネットワークの勾配計算\n",
    "\"\"\"\n",
    "# ダミー教師データ: 0.0〜1.0 の10サイズデータ 100枚分\n",
    "y = rand(Float64, 100, 10)\n",
    "\n",
    "# 勾配計算\n",
    "## @timeマクロで時間計測してみると分かるが、ニューロンの数だけ勾配計算する今の方法では非常に多くの時間がかかる\n",
    "## => Intel(C) Core i7-7700 3.6 GHz で 60〜80秒程度かかる\n",
    "## => この部分の高速化（誤差逆伝搬法）については後述する\n",
    "grad = @time numeric_gradient(net, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ミニバッチ学習の実装\n",
    "上記で実装したTwoLayerNetworkに対して、MNISTデータセットを使ってミニバッチ学習を実装する\n",
    "\n",
    "今回は、ミニバッチサイズを100として、毎回60,000個の訓練データからランダムに100個のデータを抜き出して学習することにする\n",
    "\n",
    "この100個のミニバッチを対象に勾配を求め、確率的勾配降下法（SGD）によりパラメータを更新する\n",
    "\n",
    "さらにパラメータ更新を10,000回繰り返し、損失関数の値の推移をグラフで表す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82.919140 seconds (149.05 M allocations: 107.032 GiB, 13.88% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " 4.916103202232928"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLDatasetsパッケージのMNISTSデータセットを使う\n",
    "using MLDatasets\n",
    "\n",
    "# 訓練用画像データと教師データをロード\n",
    "## train_x: 特徴量＝<画像データ｜28x28 グレースケール画像 60,000枚>{28x28x60000 Array{UInt8, 3}}\n",
    "## train_y: 目的変数＝<数値クラス｜[0..9]の数値 60,000個>{60000 Array{Int, 1}}\n",
    "train_x, train_y = MNIST.traindata()\n",
    "\n",
    "# 学習データをニューラルネットワーク用に前処理\n",
    "train_x = Array{Float64,3}(reshape(train_x, 1, 28*28, :)) # 1 x 784 x 60000 Array{Float64,3}\n",
    "train_x = permutedims(train_x, [3, 2, 1]) # 60000 x 784 x 1 Array{Float64,3}\n",
    "train_x = Array{Float64,2}(reshape(train_x, :, 784)) # 60000 x 784 Array{Float64,2}\n",
    "\n",
    "# 教師データをone-hot-vector形式に変換\n",
    "train_y = hcat([[i-1 == y ? 1.0 : 0.0 for i in 1:10] for y in train_y]...)' # 60000 x 10 Array{Float64,2}\n",
    "\n",
    "# 損失関数の履歴\n",
    "train_loss_list = []\n",
    "\n",
    "# ハイパーパラメータ\n",
    "iters_num = 1 # パラメータ更新回数\n",
    "train_size = size(train_x, 1) # 学習データ枚数: 60,000\n",
    "batch_size = 100 # ミニバッチサイズ\n",
    "learning_rate = 0.1 # 学習率\n",
    "\n",
    "# 2層ニューラルネットワーク\n",
    "net = TwoLayerNetwork()\n",
    "\n",
    "# 学習関数\n",
    "## Juliaの慣習で、副作用のある（実行ごとに結果が変わる）関数には ! をつける\n",
    "train!(net::Network) = begin\n",
    "    # ミニバッチ取得: 対象データ群から batch_size 個のデータを抜き出し\n",
    "    batch_mask = rand(1:train_size, batch_size)\n",
    "    batch_x = train_x[batch_mask, :]\n",
    "    batch_t = train_y[batch_mask, :]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    grad = numeric_gradient(net, batch_x, batch_t)\n",
    "    \n",
    "    # パラメータの更新\n",
    "    net.b[1] -= learning_rate * grad[\"B1\"]\n",
    "    net.b[2] -= learning_rate * grad[\"B2\"]\n",
    "    net.w[1] -= learning_rate * grad[\"W1\"]\n",
    "    net.w[2] -= learning_rate * grad[\"W2\"]\n",
    "    \n",
    "    # 学習経過の記録\n",
    "    loss_value = loss(net, batch_x, batch_t)\n",
    "    push!(train_loss_list, loss_value)\n",
    "end\n",
    "\n",
    "# 学習実行\n",
    "## iters_num = 1回で 80秒程度かかるため、10,000回実行しようとすると 222時間（≒9日）程度かかる\n",
    "## (Intel(C) Core i7-7700 3.6 GHz の場合)\n",
    "@time train!(net)\n",
    "train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
