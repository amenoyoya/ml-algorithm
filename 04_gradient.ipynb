{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワーク学習処理\n",
    "\n",
    "## 復習｜勾配計算\n",
    "\n",
    "まずは、シンプルなニューラルネットワークを構築し、正しく勾配計算されるか確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Array{Float64,2}:\n",
       " 0.2  0.2  -0.4\n",
       " 0.3  0.3  -0.6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@test: 参考書との比較（正しく勾配計算されるか確認）\n",
    "\"\"\"\n",
    "# ニューラルネットワーク実装読み込み\n",
    "include(\"./lib/Neuron.jl\")\n",
    "include(\"./lib/Functions.jl\")\n",
    "\n",
    "# 数値微分による勾配計算\n",
    "## 関数(Array{Float64,2})::Float64, 入力値::Array{Float64,2} -> 勾配::Array{Float64,2}\n",
    "numeric_gradient(f, x::Array{Float64,2})::Array{Float64,2} = begin\n",
    "    h = 1e-4 # 10^(-4)\n",
    "    grad = Array{Float64, 2}(undef, size(x, 1), size(x, 2)) # xと同じ次元の行列を生成\n",
    "    # 各変数ごとの数値微分を行列にまとめる\n",
    "    for row in 1:size(x, 1), col in 1:size(x, 2)\n",
    "        # 指定indexの変数に対する中心差分を求める\n",
    "        org = x[row, col]\n",
    "        x[row, col] = org + h\n",
    "        f1 = f(x) # f([..., x[row, col] + h, ...]) -> Float64\n",
    "        x[row, col] = org - h\n",
    "        f2 = f(x) # f([..., x[row, col] - h, ...]) -> Float64\n",
    "        grad[row, col] = (f1 - f2) / 2h # (row, col)番目の変数に対する数値微分\n",
    "        x[row, col] = org # x[i]の値をもとに戻す\n",
    "    end\n",
    "    return grad\n",
    "end\n",
    "    \n",
    "# シンプルなニューラルネットワーク\n",
    "SimpleNet() = Network(1,\n",
    "    [\n",
    "        zeros(1, 3) # 1 x 3 Array{Float64,2} bias_1 [0 0 0]\n",
    "    ],\n",
    "    [\n",
    "        zeros(2, 3) # 2 x 3 Array{Float64,2} weight_1 [0 0 0; 0 0 0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 活性関数を使わず、ソフトマックス関数で出力するだけの推論処理\n",
    "predict(net::Network, x::Array{Float64,2})::Array{Float64,2} = softmax(x * net.w[1] + net.b[1])\n",
    "\n",
    "# 損失関数: 交差エントロピー誤差\n",
    "loss(net::Network, x::Array{Float64,2}, t::Array{Float64,2})::Float64 = cross_entropy_error(predict(net, x), t)\n",
    "\n",
    "x = [0.6 0.9]\n",
    "t = [0.0 0.0 1.0]\n",
    "\n",
    "net = SimpleNet()\n",
    "\n",
    "# 勾配計算\n",
    "## 2 x 3 Array{Float64,2} [0.2 0.2 -0.4; 0.3 0.3 -0.6] になればOK\n",
    "dW = numeric_gradient(w->loss(net, x, t), net.w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習アルゴリズムの実装\n",
    "\n",
    "これまでに実装した「損失関数」「ミニバッチ」「勾配」「勾配降下法」をまとめることで、ニューラルネットワークの学習アルゴリズムを実装することができる\n",
    "\n",
    "### 確率的勾配降下法\n",
    "ニューラルネットワークの学習手順は以下のようなものが基本となる\n",
    "\n",
    "1. ミニバッチ\n",
    "    - 訓練データからランダムに一部のデータを選び出す（ミニバッチ）\n",
    "    - 一回の学習においては、このミニバッチの損失関数の値を減少させることを目的とする\n",
    "2. 勾配\n",
    "    - ミニバッチの損失関数を減らすために、各重みパラメータの勾配を算出する\n",
    "    - 勾配は、損失関数の値を最も減らす方向を示す\n",
    "3. パラメータの更新\n",
    "    - 重みパラメータを勾配方向に微小量だけ更新する\n",
    "4. 1.に戻って同様の手順を繰り返す\n",
    "\n",
    "ここで、使用する訓練データをミニバッチとして無作為に選び出していることから、このような学習方法を**確率的勾配降下法**（Stochastic Gradient Descent）と呼ぶ\n",
    "\n",
    "ディープラーニングの多くのフレームワークでは、確率的勾配降下法の頭文字をとって**SGD**という名前の関数で実装されているのが一般的である\n",
    "\n",
    "### 2層ニューラルネットワークの実装\n",
    "今回は、手書き数字の学習を行うためのニューラルネットワークとして、2層ニューラルネットワーク（隠れ層1つのニューラルネットワーク）を実装することにする\n",
    "\n",
    "ネットワーク設計は以下の通りとする\n",
    "\n",
    "- 入力層:\n",
    "    - 手書き数字の画像データ（サイズ: 28x28）\n",
    "    - ニューロン数: 28 * 28 = 784\n",
    "    - 各ニューロンの入力値は 0.0〜1.0 の実数型である必要がある\n",
    "- 中間層（隠れ層）1:\n",
    "    - ニューロン数: 100\n",
    "    - 活性化関数: シグモイド関数\n",
    "- 出力層:\n",
    "    - ニューロン数: 10（0〜9の数字クラスに分類するため）\n",
    "    - 活性化関数: ソフトマックス関数\n",
    "    - 損失関数: 交差エントロピー関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Array{Float64,1}:\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " ⋮                 \n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999\n",
       " 0.9999999999999999"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "2層ニューラルネットワークによる手書き数字の学習\n",
    "\"\"\"\n",
    "# Network構造体を継承して2層ニューラルネットワーク実装\n",
    "TwoLayerNetwork(weight_init_std::Float64=0.01) = Network(2,\n",
    "    [\n",
    "        zeros(Float64, 1, 100), # 1x100-Array{Float64,2} バイアス_1: 0行列\n",
    "        zeros(Float64, 1, 10),  # 1x10-Array{Float64,2} バイアス_2: 0行列\n",
    "    ],\n",
    "    [\n",
    "        rand(UInt8, 784, 100) * weight_init_std, # 784x100-Array{Float64,2} 重み_1: 任意整数 * weight_init_std の乱数行列\n",
    "        rand(UInt8, 100, 10) * weight_init_std, # 100x10-Array{Float64,2} 重み_2: 任意整数 * weight_init_std の乱数行列\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 推論処理\n",
    "## Network構造体, 入力信号 -> 出力信号 y\n",
    "predict(network::Network, x::Array{Float64,2})::Array{Float64,2} = predict(network, sigmoid, softmax, x)\n",
    "\n",
    "# 推論処理＋損失関数\n",
    "## Network構造体, 入力信号, 教師データ -> 交差エントロピー誤差\n",
    "loss(network::Network, x::Array{Float64,2}, t::Array{Float64,2})::Float64 = cross_entropy_error(predict(network, x), t)\n",
    "\n",
    "# 各パラメータの勾配計算\n",
    "## Network構造体, 入力信号, 教師データ -> 各パラメータの勾配行列をまとめた辞書\n",
    "numeric_gradient(network::Network, x::Array{Float64,2}, t::Array{Float64,2})::Dict{AbstractString, Array{Float64,2}} = begin\n",
    "    loss_func = w -> loss(network, x, t)\n",
    "    Dict(\n",
    "        \"B1\" => numeric_gradient(loss_func, network.b[1]),\n",
    "        \"B2\" => numeric_gradient(loss_func, network.b[2]),\n",
    "        \"W1\" => numeric_gradient(loss_func, network.w[1]),\n",
    "        \"W2\" => numeric_gradient(loss_func, network.w[2]),\n",
    "    )\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "@test: 2層ニューラルネットワークの推論処理\n",
    "\"\"\"\n",
    "# ダミー入力データ: 0.0〜1.0 の784サイズデータ 100枚分\n",
    "x = rand(Float64, 100, 784)\n",
    "\n",
    "# 推論実行\n",
    "net = TwoLayerNetwork()\n",
    "y = predict(net, x)\n",
    "\n",
    "# 各行ごとに列の合計値が1になっているか確認（softmax関数の特性の確認）\n",
    "[sum(y[row, :]) for row in 1:size(y, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66.529496 seconds (148.47 M allocations: 107.003 GiB, 3.83% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{AbstractString,Array{Float64,2}} with 4 entries:\n",
       "  \"W2\" => [0.251691 -0.0693278 … -0.000320471 -0.0215128; 0.251691 -0.0693278 ……\n",
       "  \"B2\" => [0.251691 -0.0693278 … -0.000320471 -0.0215128]\n",
       "  \"B1\" => [0.0 0.0 … 0.0 0.0]\n",
       "  \"W1\" => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@test: 2層ニューラルネットワークの勾配計算\n",
    "\"\"\"\n",
    "# ダミー教師データ: 0.0〜1.0 の10サイズデータ 100枚分\n",
    "y = rand(Float64, 100, 10)\n",
    "\n",
    "# 勾配計算\n",
    "## @timeマクロで時間計測してみると分かるが、ニューロンの数だけ勾配計算する今の方法では非常に多くの時間がかかる\n",
    "## => Intel(C) Core i7-7700 3.6 GHz で 約66秒かかる\n",
    "## => この部分の高速化（誤差逆伝搬法）については後述する\n",
    "grad = @time numeric_gradient(net, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
