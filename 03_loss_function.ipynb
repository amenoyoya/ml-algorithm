{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークの学習\n",
    "\n",
    "## データ駆動\n",
    "\n",
    "機械学習においてはデータが命である\n",
    "\n",
    "データからパターンを見つけ出し、データから答えを導く出すのが機械学習の本質である\n",
    "\n",
    "ここで、入力データからデータの本質的なパターンを導き出す変換器を**特徴量**と呼ぶ\n",
    "\n",
    "### 特徴量の抽出の歴史\n",
    "最も単純な分類である線形分類においては、特徴量を抽出する必要はなく、入力データから有限回の学習により問題を解くことが可能だった\n",
    "\n",
    "これは最初に行ったパーセプトロンによるAND, OR等の演算子の実装において証明されたように「パーセプトロンの収束原理」として知られている\n",
    "\n",
    "一方、非線形分類問題は入力データからの自動的な学習が不可能であり、特徴量を抽出する必要が出てきた\n",
    "\n",
    "当初は、特徴量の抽出からデータの学習（アルゴリズムの実装）まで全て人の手で行われており、このモデルは**エクスパートシステム**と呼ばれる\n",
    "\n",
    "その後、特徴量はデータサイエンティストと呼ばれる人々によって抽出されるようになり、抽出された特徴量を**機械学習アルゴリズム**（SVM, kNNなど）を用いて自動的に学習することが可能になった\n",
    "\n",
    "そして近年話題となった**ニューラルネットワーク（ディープラーニング）**においては、特徴量の抽出からデータの学習までを全てコンピュータによって行われるようになっている\n",
    "\n",
    "![machine_learning_history](./img/machine_learning_history.png)\n",
    "\n",
    "### パラメータの最適化と損失関数\n",
    "機械学習およびニューラルネットワークにおいては、データの学習とはすなわち**パラメータの最適化**である\n",
    "\n",
    "例えばニューラルネットワークは、重みとバイアスというパラメータの値を少しずつ変化させ、計算結果が教師データと近い値になるように学習を行っている\n",
    "\n",
    "この時、計算結果と教師データとの誤差を表現する関数を**損失関数**と呼び、この損失関数を最小化することが学習の目的となる\n",
    "\n",
    "関数の最小値を求める場合、数学的には**微分**という手法が用いられる\n",
    "\n",
    "微分値とは「ある瞬間における変化の量」と定義される値のことであり、わかりやすく言えば、グラフの接線の傾きのことである\n",
    "\n",
    "簡単のため二次関数を例にすると下図の通り、微分値が0になる（接線の傾きが0になる）点が関数の最小値点であり、パラメータの最適点であると考えることができる\n",
    "\n",
    "![loss_function_optimization](./img/loss_function_optimization.png)\n",
    "\n",
    "ニューラルネットワークの学習においてはこのように、「損失関数」と「微分」がキーワードとなる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微分の実装\n",
    "\n",
    "微分の数学的な定義は以下のようになる\n",
    "\n",
    "$$\n",
    "    \\frac{df(x)}{dx} = \\lim_{h \\to 0}{\\frac{f(x+h) - f(x)}{h}}\n",
    "$$\n",
    "\n",
    "この式は、関数 $f(x)$ において $x$ が極めて微小な量 $h$ だけ変化したときの $f(x)$ の変化量を表している\n",
    "\n",
    "ここで $x$ の極めて微小な変化量 $h$ をプログラム的にどう表すかが問題となる\n",
    "\n",
    "### 数値微分\n",
    "上記問題の最も単純な解決策は、$h$ に $10^{-50}$ などの極めて小さな値を入れてしまうことである\n",
    "\n",
    "このような微分手法を**数値微分**と呼ぶ\n",
    "\n",
    "この実装における注意点として、実装するプログラミング言語の**丸め誤差**を考慮しなければならないことと、数値微分で生じる誤差を極力減らすように工夫しなければならないことが挙げられる\n",
    "\n",
    "丸め誤差とは、小数の小さな範囲で数値が省略されることで発生する誤差のことで、計算機イプシロンとして定義されており、Juliaにおいては `2.220446049250313e-16` という値になっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 計算機イプシロンの取得\n",
    "eps(Float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理論上は、上記の値より大きな値を $h$ に設定すればよいが、計算量との兼ね合いから、一般的に $10^{-4}$ に設定すれば大抵の場合で上手く行くとされている\n",
    "\n",
    "続いて、数値微分によって生じる誤差について考える\n",
    "\n",
    "数値微分では、「限りなく0に近い値」を「極めて微小な値」と置き換えて実装するため、その分の誤差が生じる\n",
    "\n",
    "この誤差を極力減らすため、数値微分の実装では**中心差分**という差分をとることが多い\n",
    "\n",
    "ここで、微分の定義における $f(x+h) - f(x)$ のような差分を**前方差分**と呼ぶが、中心差分では $x$ を中心としてその前後の差分 $f(x+h) - f(x-h)$ をとる\n",
    "\n",
    "このように差分の平均値を計算することで、誤差を減らすことができる\n",
    "\n",
    "ここまでの注意点を考慮して数値微分を実装すると以下のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numeric_diff (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数値微分関数\n",
    "numeric_diff(f, x) = begin\n",
    "    h = 1e-4 # 10^(-4)\n",
    "    (f(x + h) - f(x - h)) / 2h # 中心差分から微分値を計算\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@test: 数値微分\n",
    "\n",
    "例題: f(x) = 0.01x^2 + 0.1x\n",
    "解析的微分: f'(x) = 0.02x + 0.1\n",
    "    => f'(5) = 0.2, f'(10) = 0.3\n",
    "\"\"\"\n",
    "# 例題の設定\n",
    "f(x) = 0.01x^2 + 0.1x\n",
    "\n",
    "# 数値微分: f'(5)\n",
    "## => 0.1999999999990898 ≒ 0.2\n",
    "println(numeric_diff(f, 5))\n",
    "\n",
    "# 数値微分: f'(10)\n",
    "## => 0.2999999999986347 ≒ 0.3\n",
    "println(numeric_diff(f, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
